# -*- coding: utf-8 -*-
"""FF LT REVERSAL V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XX3Q5irR5crwxQdtJcaR0IKJBEXPt9zX
"""

import pandas as pd
import statsmodels.formula.api as smf
import numpy as np
import matplotlib.pyplot as plt

!pip install pandasql
import os
from datetime import datetime
from sklearn.model_selection import train_test_split
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima_model import ARIMA
from sklearn.metrics import mean_squared_error, mean_absolute_error

import math
import matplotlib.pyplot as plt
from datetime import datetime
import seaborn as sns
import pandasql as ps
from sqlite3 import connect
from google.colab import drive
drive.mount("/content/gdrive", force_remount=True)

conn=connect(':memory:')

file='cleaned_returns_ltonly.csv'

filepath="/content/gdrive/MyDrive/"
daily_returns=pd.read_csv(filepath + file)

daily_returns = daily_returns.sort_values(by=["ticker", "date"])
daily_returns

daily_returns = daily_returns.sort_values(by=["ticker", "date"])

daily_returns['price_at_day_minus_251'] = daily_returns.groupby('ticker')['adj_close'].shift(251)
daily_returns['price_at_day_minus_1251'] = daily_returns.groupby('ticker')['adj_close'].shift(1251)

# Drop rows where either price is missing
daily_returns.dropna(subset=['price_at_day_minus_251', 'price_at_day_minus_1251'], inplace=True)

# Now it is safe to calculate 'lt_return'
daily_returns['lt_return'] = (daily_returns['price_at_day_minus_251'] / daily_returns['price_at_day_minus_1251']) - 1

# Drop the temporary price columns
daily_returns = daily_returns.drop(columns=['price_at_day_minus_251', 'price_at_day_minus_1251'])

daily_returns

main="/content/gdrive/MyDrive/FIMA SUMMER 2023/RISK MANAGEMENT/FAMA FRENCH FACTORS/_main/"
raw= main + '_raw/'
aux= raw + 'Cake Shop Realtime Fundamentals as of June 23, 2023/'
clean= main + '_clean/'


file="cleaned_fundamentals.csv"
fundamentals=pd.read_csv(clean + file)
fundamentals['per_end_date']=pd.to_datetime(fundamentals['per_end_date'], format="%Y-%m-%d")
daily_returns['date']=pd.to_datetime(daily_returns['date'], format="%Y-%m-%d")
#set both dates to relevant datetime format

#get the specific quarter from daily returns data--> We will subtract one quarter
#from the date, then extract the quarter

one_quarter = pd.tseries.offsets.DateOffset(months=3)
daily_returns['last_quarter']=daily_returns['date']-one_quarter
daily_returns['last_quarter']=pd.PeriodIndex(daily_returns.last_quarter, freq='Q')

#extract quarter from daily returns dataframe
fundamentals['quarter']=pd.PeriodIndex(fundamentals.per_end_date, freq='Q')


#we now merge the dataframe together using SQL

#apply strings to quarters for SQL
fundamentals['quarter']=fundamentals['quarter'].apply(str)
daily_returns['last_quarter']=daily_returns['last_quarter'].apply(str)

#send to sql
daily_returns.to_sql('dr', conn, if_exists='replace')
fundamentals.to_sql('f', conn, if_exists='replace')

#query


query='''SELECT DISTINCT dr.*, f.mkt_val
          FROM dr
          LEFT JOIN f ON dr.last_quarter=f.quarter AND dr.ticker=f.ticker
          '''

daily_returns_size=pd.read_sql(query, conn)
daily_returns=daily_returns_size

"""Size Sorts"""

f_shares=pd.read_csv(aux + 'ZACKS_SHRS_2.csv')

f_shares['per_end_date']=pd.to_datetime(f_shares['per_end_date'])
f_shares['quarter'] = f_shares['per_end_date'].dt.to_period('Q')
f_shares['quarter'] = f_shares['quarter'].astype(str)


# Merge f_shares data into daily_returns based on ticker and last_quarter
daily_returns = daily_returns.merge(f_shares[['ticker', 'quarter', 'shares_out']], left_on=['ticker', 'last_quarter'], right_on=['ticker', 'quarter'], how='left')
daily_returns['last_quarter']=daily_returns['last_quarter'].apply(str)

# Calculate market cap by multiplying closing price by shares outstanding
daily_returns['mkt_cap_2'] = daily_returns['adj_close'] * daily_returns['shares_out']

# Set market cap to previous day's market cap
daily_returns['mkt_cap_2'] = daily_returns['mkt_cap_2'].shift(1)

daily_returns.rename(columns={'lt_return': 'prior_month_ret'}, inplace=True)


#filter for relevant exchanges
#daily_returns=daily_returns[(daily_returns['exchange']=="NYSE") | (daily_returns['exchange']=="AMEX") | (daily_returns['exchange']=="NASDAQ")]
daily_returns['date']=pd.to_datetime(daily_returns['date'])

daily_returns_sorts= daily_returns.dropna(subset=['prior_month_ret', 'mkt_cap_2'])

#filter for NYSE stocks only
return_breakpoints=daily_returns_sorts[daily_returns['exchange']=="NYSE"]


q30_ret=return_breakpoints.groupby('date').apply(lambda x: np.quantile(x.prior_month_ret, 0.3))
q70_ret=return_breakpoints.groupby('date').apply(lambda x: np.quantile(x.prior_month_ret, 0.7))


#format these values as dataframes to match together

q30_ret=pd.DataFrame(q30_ret)
q30_ret=q30_ret.rename(columns={0: 'q30'})

q70_ret=pd.DataFrame(q70_ret)
q70_ret=q70_ret.rename(columns={0: 'q70'})

return_quantiles=pd.merge(q30_ret, q70_ret, on='date')
return_quantiles

# Merge return_quantiles data into daily_returns based on date
return_quantiles.reset_index(inplace=True)
return_quantiles['date']=pd.to_datetime(return_quantiles['date'])
daily_returns = daily_returns_sorts.merge(return_quantiles[['date', 'q30', 'q70']], on='date', how='left')

#assign to relevant portfolios
daily_returns['return_portfolio']=np.where(daily_returns['prior_month_ret'] > daily_returns['q70'], 'H', np.where(daily_returns['q30']> daily_returns['prior_month_ret'], 'L', 'M'))

size_breakpoints=daily_returns[daily_returns['exchange']=="NYSE"]
median_size=size_breakpoints.groupby('date').apply(lambda x: np.quantile(x.mkt_cap_2, 0.5))
median_size=pd.DataFrame(median_size)
median_size=median_size.rename(columns={0: 'median_size'})
median_size.reset_index(inplace=True)
median_size['date']=pd.to_datetime(median_size['date'])
# Merge median_size data into daily_returns_sorts based on date
daily_returns = daily_returns.merge(median_size[['date', 'median_size']], on='date', how='left')

daily_returns['size_portfolio']=np.where(daily_returns['mkt_cap_2']>daily_returns['median_size'], 'B', 'S')

daily_returns=daily_returns[['ticker', 'date', 'adj_close', 'daily_return', 'prior_month_ret', 'mkt_cap_2', 'return_portfolio', 'size_portfolio']]
daily_returns.rename(columns={'prior_month_ret': 'prior_daily_ret', 'mkt_cap_2':'mkt_val'}, inplace=True)
main=daily_returns

valueWeightRet=main.groupby(['date', 'size_portfolio', 'return_portfolio']).apply(lambda x: np.average(pd.to_numeric(x['daily_return']), weights=pd.to_numeric(x['mkt_val'])))


valueWeightRet_df=pd.DataFrame(valueWeightRet)
valueWeightRet_df=valueWeightRet_df.reset_index(level=['size_portfolio', 'return_portfolio'])
portfolios=valueWeightRet_df

portfolios

portfolios['size_return_portfolio']=portfolios['size_portfolio'] + "/" + portfolios['return_portfolio']

portfolios=portfolios.drop(['size_portfolio', 'return_portfolio'], axis=1)
portfolios=pd.pivot(portfolios, columns='size_return_portfolio', values= 0)
print(portfolios)

portfolios.reset_index(inplace=True)
portfolios['date']=pd.to_datetime(portfolios['date'])

portfolios

portfolios['lt_rev']=0.5 * (portfolios['S/L'] + portfolios['B/L']) - 0.5 * (portfolios['S/H']+portfolios['B/H'])

LT=portfolios[['date', 'lt_rev']]

LT

import pandas_datareader.data as web
from pandas_datareader.famafrench import get_available_datasets
datasets = get_available_datasets()
datasets


FF_LT_Factor=[dataset for dataset in datasets if 'F-F_LT_Reversal_Factor_daily' in dataset ]

FF_LT_Factor=web.DataReader(FF_LT_Factor[0],'famafrench',start='2017-11-01',end='2023-01-01')[0]

FF_LT_Factor.reset_index(inplace=True)
FF_LT_Factor['Date']=pd.to_datetime(FF_LT_Factor['Date'])
FF_LT_Factor

merged_df = LT.merge(FF_LT_Factor, left_on='date', right_on='Date', how='left')

merged_df.corr()

checker=merged_df

import matplotlib.pyplot as plt

# Assuming checker is a pandas DataFrame containing your time series data
# And 'date' is a column in checker containing the dates of each data point

plt.figure(figsize=(14, 7))
plt.plot(checker['date'], checker['lt_rev'], label='Reconstruction of LT Reversal factor')
plt.plot(checker['date'], checker['LT_Rev'], label='Fama French LT Reversal Factor')
plt.title('Reconstruction of LT Reversal vs Original FF LT Reversal')
plt.xlabel('Date')
plt.ylabel('Factor Value')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Create scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(checker['lt_rev'], checker['LT_Rev'], alpha=0.5)

# Add line of perfect correlation
limits = [np.min([plt.xlim(), plt.ylim()]), np.max([plt.xlim(), plt.ylim()])]  # get the current limits
plt.plot(limits, limits, color='red')  # plot a line from the lower left to the upper right

# Set the limits again
plt.xlim(limits)
plt.ylim(limits)

# Adding labels
plt.xlabel('Reconstruction of LT Reversal factor')
plt.ylabel('Fama French LT Reversal Factor')
plt.title('Scatter plot with Line of Perfect Correlation')

plt.show()