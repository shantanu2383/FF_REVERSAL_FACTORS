# -*- coding: utf-8 -*-
"""FF ST REVERSAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CWSwqRS6CEivjgK7ON5pzG1DMxUdIegX
"""

import pandas as pd
from datetime import datetime
from google.colab import drive
drive.mount("/content/gdrive", force_remount=True)

#SET PATHS
main="/content/gdrive/MyDrive/FIMA SUMMER 2023/RISK MANAGEMENT/FAMA FRENCH FACTORS/_main/"
raw= main + '_raw/'
aux= raw + 'Cake Shop Realtime Fundamentals as of June 23, 2023/'
clean= main + '_clean/'

"""# 01. Import Cleaned Price Data"""

file="cleaned_returns_only.csv"

daily_returns=pd.read_csv(clean + file)

"""# 02. Calculate ST Reversal Return"""

daily_returns = daily_returns.sort_values(by=["ticker", "date"])

daily_returns['adj_close_shifted_1'] = daily_returns.groupby('ticker')['adj_close'].shift(2)
daily_returns['adj_close_shifted_21'] = daily_returns.groupby('ticker')['adj_close'].shift(21)

daily_returns['prior_month_ret'] = (daily_returns['adj_close_shifted_1'] / daily_returns['adj_close_shifted_21']) - 1

daily_returns = daily_returns.drop(columns=['adj_close_shifted_1', 'adj_close_shifted_21'])
daily_returns.dropna(subset=['prior_month_ret'], inplace=True)

"""# 03. Import Fundamentals Data"""

file="cleaned_fundamentals.csv"
fundamentals=pd.read_csv(clean + file)

"""# 04.  Market Cap For Weighted Returns"""

fundamentals['per_end_date']=pd.to_datetime(fundamentals['per_end_date'], format="%Y-%m-%d")
daily_returns['date']=pd.to_datetime(daily_returns['date'], format="%Y-%m-%d")
#set both dates to relevant datetime format

#get the specific quarter from daily returns data--> We will subtract one quarter
#from the date, then extract the quarter

one_quarter = pd.tseries.offsets.DateOffset(months=3)
daily_returns['last_quarter']=daily_returns['date']-one_quarter
daily_returns['last_quarter']=pd.PeriodIndex(daily_returns.last_quarter, freq='Q')

#extract quarter from daily returns dataframe
fundamentals['quarter']=pd.PeriodIndex(fundamentals.per_end_date, freq='Q')


#we now merge the dataframe together using SQL

#apply strings to quarters for SQL
fundamentals['quarter']=fundamentals['quarter'].apply(str)
daily_returns['last_quarter']=daily_returns['last_quarter'].apply(str)

daily_returns.merge(fundamentals[['quarter', 'ticker', 'mkt_val']], left_on=['last_quarter', 'ticker'], right_on=['quarter', 'ticker'], how='left')

print(daily_returns)

"""# 04. Calculate Proxy for Daily Market Cap

We extract the common shares outstanding at the end of each quarter from Fundamental data. We then merge these into our daily returns dataframe, so we will have the shares oustanding for the previous quarter for each date. This allows us to calculate an alternate measure of market cap as (close_price * (shares outstanding at last quarter). By doing this, this measure of market cap will be able to vary by day, which makes them more relevant for calculating daily size portfolios. THIS IS A PROXY FOR DAILY MARKET CAPITALISATION
"""

f_shares=pd.read_csv(aux + 'ZACKS_SHRS_2.csv')

f_shares['per_end_date']=pd.to_datetime(f_shares['per_end_date'])
f_shares['quarter'] = f_shares['per_end_date'].dt.to_period('Q')
f_shares['quarter'] = f_shares['quarter'].astype(str)


# Merge f_shares data into daily_returns based on ticker and last_quarter
daily_returns = daily_returns.merge(f_shares[['ticker', 'quarter', 'shares_out']], left_on=['ticker', 'last_quarter'], right_on=['ticker', 'quarter'], how='left')
daily_returns['last_quarter']=daily_returns['last_quarter'].apply(str)

# Calculate market cap by multiplying closing price by shares outstanding
daily_returns['mkt_cap_2'] = daily_returns['adj_close'] * daily_returns['shares_out']

# Set market cap to previous day's market cap
daily_returns['mkt_cap_2'] = daily_returns['mkt_cap_2'].shift(1)

"""# 05. Daily Return Sort

In this section we sort on daily prior returns:
The methodology is as follows:



1.   Get subset of NYSE stocks only
2.   Calculate breakpoint of NYSE stocks for each date
3.   Sort into prior return portfolios based on prior daily return cf breakpoints
"""

import numpy as np
#filter for relevant exchanges
daily_returns=daily_returns[(daily_returns['exchange']=="NYSE") | (daily_returns['exchange']=="AMEX") | (daily_returns['exchange']=="NASDAQ")]
daily_returns['date']=pd.to_datetime(daily_returns['date'])

daily_returns_sorts= daily_returns.dropna(subset=['prior_month_ret', 'mkt_cap_2'])

#filter for NYSE stocks only
return_breakpoints=daily_returns_sorts[daily_returns['exchange']=="NYSE"]


q30_ret=return_breakpoints.groupby('date').apply(lambda x: np.quantile(x.prior_month_ret, 0.3))
q70_ret=return_breakpoints.groupby('date').apply(lambda x: np.quantile(x.prior_month_ret, 0.7))


#format these values as dataframes to match together

q30_ret=pd.DataFrame(q30_ret)
q30_ret=q30_ret.rename(columns={0: 'q30'})

q70_ret=pd.DataFrame(q70_ret)
q70_ret=q70_ret.rename(columns={0: 'q70'})

return_quantiles=pd.merge(q30_ret, q70_ret, on='date')

# Merge return_quantiles data into daily_returns based on date
return_quantiles.reset_index(inplace=True)
return_quantiles['date']=pd.to_datetime(return_quantiles['date'])
daily_returns = daily_returns_sorts.merge(return_quantiles[['date', 'q30', 'q70']], on='date', how='left')

#assign to relevant portfolios
daily_returns['return_portfolio']=np.where(daily_returns['prior_month_ret'] > daily_returns['q70'], 'H', np.where(daily_returns['q30']> daily_returns['prior_month_ret'], 'L', 'M'))

"""# 07. Daily Size Sorts"""

size_breakpoints=daily_returns[daily_returns['exchange']=="NYSE"]
median_size=size_breakpoints.groupby('date').apply(lambda x: np.quantile(x.mkt_cap_2, 0.5))
median_size=pd.DataFrame(median_size)
median_size=median_size.rename(columns={0: 'median_size'})
median_size.reset_index(inplace=True)
median_size['date']=pd.to_datetime(median_size['date'])
# Merge median_size data into daily_returns_sorts based on date
daily_returns = daily_returns.merge(median_size[['date', 'median_size']], on='date', how='left')

daily_returns['size_portfolio']=np.where(daily_returns['mkt_cap_2']>daily_returns['median_size'], 'B', 'S')

daily_returns.columns

"""Export for other factors

# 08. Portfolio Construction
"""

daily_returns=daily_returns[['ticker', 'date', 'adj_close', 'daily_return', 'prior_month_ret', 'mkt_cap_2', 'return_portfolio', 'size_portfolio']]
daily_returns.rename(columns={'prior_month_ret': 'prior_daily_ret', 'mkt_cap_2':'mkt_val'}, inplace=True)
main=daily_returns

valueWeightRet=main.groupby(['date', 'size_portfolio', 'return_portfolio']).apply(lambda x: np.average(pd.to_numeric(x['daily_return']), weights=pd.to_numeric(x['mkt_val'])))


valueWeightRet_df=pd.DataFrame(valueWeightRet)
valueWeightRet_df=valueWeightRet_df.reset_index(level=['size_portfolio', 'return_portfolio'])
portfolios=valueWeightRet_df

portfolios[0]=portfolios[0]-1
portfolios[0]*=100


x=['size_portfolio', 'return_portfolio']

for x in x:
  portfolios[x]=portfolios[x].apply(str)

portfolios['size_return_portfolio']=portfolios['size_portfolio'] + "/" + portfolios['return_portfolio']

portfolios=portfolios.drop(['size_portfolio', 'return_portfolio'], axis=1)

portfolios=pd.pivot(portfolios, columns='size_return_portfolio', values=0)

portfolios
x=['B/H', 'B/L', 'B/M', 'S/H', 'S/L', 'S/M']
for x in x:
  portfolios[x]=pd.to_numeric(portfolios[x], errors='coerce')

portfolios['ST_rev']=0.5 * (portfolios['S/L'] + portfolios['B/L']) - 0.5 * (portfolios['S/H']+portfolios['B/H'])

"""# Import Fama French Data for Comparison"""

import pandas_datareader.data as web
from pandas_datareader.famafrench import get_available_datasets
datasets = get_available_datasets()
datasets

FF_ST_Factor=[dataset for dataset in datasets if 'F-F_ST_Reversal_Factor_daily' in dataset ]

FF_ST_Factor=web.DataReader(FF_ST_Factor[0],'famafrench',start='2017-11-01',end='2023-01-01')[0]

FF_ST_Factor.reset_index(inplace=True)
FF_ST_Factor['Date']=pd.to_datetime(FF_ST_Factor['Date'])
FF_ST_Factor

portfolios.reset_index(inplace=True)
portfolios['date']=pd.to_datetime(portfolios['date'])
ST=portfolios[['date', 'ST_rev']]
checker = ST.merge(FF_ST_Factor, left_on='date', right_on='Date', how='left')

checker['ST_rev']=checker['ST_rev']/100

checker

checker.corr()

checker.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])

import matplotlib.pyplot as plt

# Assuming checker is a pandas DataFrame containing your time series data
# And 'date' is a column in checker containing the dates of each data point

plt.figure(figsize=(14, 7))
plt.plot(checker['date'], checker['ST_rev'], label='Reconstruction of ST Reversal factor')
plt.plot(checker['date'], checker['ST_Rev'], label='Fama French ST Reversal Factor')
plt.title('Reconstruction of ST Reversal vs Original FF ST Reversal')
plt.xlabel('Date')
plt.ylabel('Factor Value')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Create scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(checker['ST_rev'], checker['ST_Rev'], alpha=0.5)

# Add line of perfect correlation
limits = [np.min([plt.xlim(), plt.ylim()]), np.max([plt.xlim(), plt.ylim()])]  # get the current limits
plt.plot(limits, limits, color='red')  # plot a line from the lower left to the upper right

# Set the limits again
plt.xlim(limits)
plt.ylim(limits)

# Adding labels
plt.xlabel('Reconstruction of ST Reversal factor')
plt.ylabel('Fama French ST Reversal Factor')
plt.title('Scatter plot with Line of Perfect Correlation')

plt.show()

"""# Export Clean Factors"""

main="/content/gdrive/MyDrive/FIMA SUMMER 2023/RISK MANAGEMENT/FAMA FRENCH FACTORS/"

output= main + '_output/'

ST.to_csv(output + 'Clean ST Reversal Factor.csv')